{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Classification and Generation\n",
    "\n",
    "[Martina Cioffi](https://github.com/martinacioffi) – 3010036\n",
    "\n",
    "[Edoardo Manieri](https://github.com/edoardomanieri) – 3084469\n",
    "\n",
    "[Valentina Parietti](https://github.com/ValentinaParietti) – 3007385\n",
    "\n",
    "[Edoardo Pericoli](https://github.com/Edoardopericoli) –  3001596\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Datasets](#datasets)\n",
    "    1. [Stanford Dataset](#stanford)\n",
    "    2. [Our Dataset](#our)\n",
    "\n",
    "2. [Classification](#classification)\n",
    "    1. [From Scratch](#scratch)\n",
    "    2. [Tranfer Learning](#tranferleraning)\n",
    "        1. [EfficientNet B1](#efficientnetB1)\n",
    "        2. [EfficientNet B7](#efficientnetB7)\n",
    "    3. [Object Detection](#objectdetection)\n",
    "    \n",
    "3. [Generation](#generation)\n",
    "\n",
    "    1. [Data Preparation](#datapreparation)\n",
    "    2. [StyleGAN](#stylegan)\n",
    "    \n",
    "4. [Profiling](#profile)\n",
    "\n",
    "5. [Guess the Car](#game)\n",
    "\n",
    "6. [References](#refs)\n",
    "\n",
    "Please, note that the whole code, together with a more detailed explanation on how to run it can be found on GitHub at the following [link](https://github.com/Edoardopericoli/Car_Prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets <a name=\"datasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main idea for this project was to train a model to be able to classify cars starting from pictures of them. Following is a brief explanation of the steps followed both in terms of the collection and building of the datasets and of the model(s) used for classification. Moreover, we also generate some new images of cars starting from our own pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Stanford Dataset <a name=\"stanford\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first trial consisted in trying to predict the make, model and year of a car using images from the [Stanford Dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). This contains slightly more than 16,000 images of which, however, only half are labelled. Therefore, in order to train our initial model, we used those 8,144 images.\n",
    "\n",
    "The dataset contains 196 classes; below is a graphical representation of the distirbution of the different brands. Note, however, that the graph below does not imply imbalance between the classes: indeed, we predict the car's model and year rather than merely the make. Still, given that cars of the same make are inevitably more similar between each other than cars from different makes, the picture is useful in understanding the difficulty of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphic_sources/brands_stanford.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, see the [Classification](#classification) section for a summary of the obtained results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Our Dataset <a name=\"our\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given, however, that the Stanford dataset has relatively few images per class, and a very high number of classes, we decided to build a new dataset from scratch, containing (i) cars we were more familiar with (i.e. mostly sold in Europe rather than in the United States), and (ii) more images per class (eventually, we had around 200 images on average per each car's model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows the distribution of brands for our final dataset.\n",
    "\n",
    "<img src=\"graphic_sources/brands_our.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification <a name=\"classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we first approached the classification problem we did it using the Stanford Dataset that was mentioned in the previous section. It is worth reminding that the dataset contains 196 classes and 8,144 images.\n",
    "\n",
    "It is also important to explain that we only provide the code for the models that we ran on our dataset because we consider what we did before an experimental phase. Therefore, here below you will find some results related to that experimental phase followed by the explanation and the codes for the actual classication models ran on our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. From Scratch <a name=\"scratch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we wanted to have ready was a baseline model to eventually enrich with more tuning or with more complex architectures. Therefore, we\n",
    "created a model from scratch and adjusted some parameters to increase the accuracy. An overview of the last version of the architecture is showed below:\n",
    "\n",
    "<img src=\"graphic_sources/baseline_architecture.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model run on 196 classes with 60 epochs gave us the following performance:\n",
    "\n",
    "<img src=\"graphic_sources/evaluation_baseline.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stopped the model after 17 epochs since after that it started to overfit. We obtained an accuracy of __3%__ against an accuracy of __0.05%__ of random guessing among 196 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Transfer Learning <a name=\"tranferleraning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to improve the accuracy, since this layout was not capable of getting deeper patterns. We therefore decided to use __transfer learning__. In particular the first\n",
    "architecture we decided to apply was the EfficientNets family. We took ImageNet pretrained checkpoints and finetuned on our dataset.\n",
    "__EfficientNets__, in particular, rely on AutoML and compound scaling to achieve superior performance without compromising resource efficiency. This is a feature that makes them really powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 EfficientNet B1 <a name=\"efficientnetB1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Stanford Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpler architecture of this family was the EffNet B1. In particular we decided to use it to improve the model on the Stanford Dataset. After fine tuning it on the last layers, we obtained the following performance:\n",
    "\n",
    "<img src=\"graphic_sources/evaluation_effnet1_stanford.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this architecture really improved the accuracy. We moved from __3%__ to __60%__. What we can also see is that after __8 epochs__ it tended to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first experimental phase we decided to build our dataset with images taken from Google and gathering more images for each model (as explained in the previous section). \n",
    "\n",
    "We first used EfficientNet B1 on about 4000 images and 20 classes (and in the graph below you can see the performances) and then we enriched the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the performances we obtained on the above model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphic_sources/evaluation_effnet1_newdata.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained __80%__ of accuracy after __3 epochs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following commands to train the B1 model on the full new dataset (10000 images and 40 classes). You can find the complete documentation of the parameters in the README at this [link](https://github.com/Edoardopericoli/Car_Prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTENTION! Running only on server\n",
    "# !python train_main.py --username='trial' --net='EffnetB1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 EfficientNet B7 <a name=\"efficientnetB7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we decided to apply to our dataset the last version of EfficientNet, that is the B7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following commandas to train the B7 model on the full new dataset (10000 images and 40 classes). You can find the complete documentation of the parameters in the README at this [link](https://github.com/Edoardopericoli/Car_Prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTENTION! Running only on server\n",
    "# !python train_main.py --username='trial' --net='EffnetB7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the performance we obtained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphic_sources/evaluation_effnet7_newdata.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above we can see an accuracy of __90%__ after __4 epochs__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Object Detection <a name=\"objectdetection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only look once ([YOLO](https://pjreddie.com/darknet/yolo/)) is a state-of-the-art, real-time object detection system. \n",
    "\n",
    "We decided to try and implement it into our pipeline since we thought that it might improve prediction. Our idea was to use YOLO to detect the bounding box of the biggest car in the image and use it to crop the image. By feeding the cropped image to the network we thought we might be eliminating some of the noise created by image backgound, other cars in the image, other objects in the image.\n",
    "\n",
    "Our first approach has been to try and implement YOLO using [Darknet](https://pjreddie.com/darknet/) but we found iterating over different images difficult and we also had issues trying to extract the coordinates of the bounding boxes. \n",
    "\n",
    "As a second approach we decided to use [ImageAI](https://imageai.readthedocs.io/en/latest/detection/index.html) which turned out to be much simpler.\n",
    "\n",
    "An example of the reasoning that we tried to implement is shown in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphic_sources/yolo.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to apply object detection to the images and running the EfficientNet B7 on the cropped images, execute the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTENTION! Running only on server\n",
    "# !python train_main.py --username='trial_YOLO' --net='EffnetB7' --crop_images=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Generation <a name=\"generation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Preparation <a name=\"datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read raw images \n",
    "\n",
    "files = os.listdir('data/raw_data/StyleGAN/StyleGAN_raw')\n",
    "files.sort()\n",
    "files=files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that adds white borders to non square images and rescales them to 256x256\n",
    "\n",
    "def make_square(im, min_size=256, fill_color=(255, 255, 255, 0)):\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the make_square function to the raw images and create a folder with the final images \n",
    "\n",
    "for i in files:\n",
    "    im = Image.open('data/raw_data/StyleGAN/StyleGAN_raw/'+str(i))\n",
    "    new_im=make_square(im)\n",
    "    new_size=(256,256)\n",
    "    new_im = new_im.resize(new_size)\n",
    "    new_im.save('data/raw_data/StyleGAN/StyleGAN_final/'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan'...\n",
      "remote: Enumerating objects: 419, done.\u001b[K\n",
      "remote: Total 419 (delta 0), reused 0 (delta 0), pack-reused 419\u001b[K\n",
      "Receiving objects: 100% (419/419), 20.69 MiB | 3.60 MiB/s, done.\n",
      "Resolving deltas: 100% (245/245), done.\n"
     ]
    }
   ],
   "source": [
    "#Clone the repository needed to generate cars from our dataset\n",
    "\n",
    "!git clone https://github.com/ValentinaParietti/stylegan.git #This repository has been forked from the\n",
    "                                                             #original StyleGAN repository and some changes have\n",
    "                                                             #been made to it in order to run StyleGAN on our dataset. \n",
    "                                                             #More on this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"data/raw_data/StyleGAN/StyleGAN_final\"\n",
      "Creating dataset \"stylegan/datasets/custom_datasets\"\n",
      "Added 2108 images.                      \n"
     ]
    }
   ],
   "source": [
    "#Run the following command to convert the images into .tfrecords (format required by StyleGAN)\n",
    "\n",
    "!python stylegan/dataset_tool.py create_from_images stylegan/datasets/custom_datasets data/raw_data/StyleGAN/StyleGAN_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: datasets/ (stored 0%)\n",
      "  adding: datasets/custom_datasets/ (stored 0%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r04.tfrecords (deflated 38%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r02.tfrecords (deflated 48%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r08.tfrecords (deflated 48%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r05.tfrecords (deflated 40%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r03.tfrecords (deflated 41%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r06.tfrecords (deflated 43%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r07.tfrecords (deflated 47%)\n"
     ]
    }
   ],
   "source": [
    "#Zip the newly filled datasets folder\n",
    "\n",
    "os.chdir('stylegan')\n",
    "!zip -r datasets_zip datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. StyleGAN <a name=\"stylegan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StyleGAN](https://github.com/NVlabs/stylegan) is an alternative generator architecture for generative adversarial networks created by NVIDIA. It borrows from style transfer literature and it creates the artificial image gradually, starting from a very low resolution and moving to a high resolution. StyleGAN modifies the input of each level separately and this allows control over the features that are expressed in that level, from coarse features (i.e. orientation, shape) to details (i.e. colour), without affecting other levels. Finally, it allows for better understanding of the generated output and produces high-resolution images that look more authentic than previously generated images\n",
    "\n",
    "In order to train StyleGAN and generate new images a GPU is needed.\n",
    "\n",
    "Therefore, the rest of the code for our generation task can be found on the Google Colab Notebook at this link: https://colab.research.google.com/drive/1FE9GBqh0qBQ8nUDDIDjWhy5R2sdgiqD0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Profiling <a name=\"profile\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphic_sources/heatmap.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the picture above, the functions _shutil.copy_ and _shutil.rmtree_,which copy the images in train test and validation folders, are really computationally expensive. One possible solution could be to let the model get the data directly from the _cars_train_new_ folder. Neverthless we decided not to make this change, because we wanted to mantain a cleaner separation among the folders.\n",
    "\n",
    "If you want a complete overview of the profiling of our code you can run the following command (please, pay attention to the cache of your browser if you run all of them sequentially):\n",
    "\n",
    "`!vprof --input-file profiling/heatmap.json`\n",
    "\n",
    "`!vprof --input-file profiling/profiler.json`\n",
    "\n",
    "`!vprof --input-file profiling/memory.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Guess the Car <a name=\"game\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to try and challenge our model, we developed a little game that allows users to try to guess the make and model of a randomly displayed car from our dataset, and reports failure or success, as well as the true car model and the prediction of our net.\n",
    "\n",
    "You can play by running the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd guess-make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 app.py -path static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References <a name=\"refs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**3D Object Representations for Fine-Grained Categorization**](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei.\n",
    "*4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13).* Sydney, Australia. Dec. 8, 2013.\n",
    "\n",
    "[**A Style-Based Generator Architecture for Generative Adversarial Networks**](https://arxiv.org/abs/1812.04948). Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)\n",
    "\n",
    "[**EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks**](https://arxiv.org/abs/1905.11946). Mingxing Tan, Quoc V. Le (Google Research, Brain Team, Mountain View, CA.)\n",
    "\n",
    "[**YOLOv3: An Incremental Improvement**](https://arxiv.org/abs/1804.02767). Joseph Redmon, Ali Farhadi. 2018. *arXiv:1804.02767*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
