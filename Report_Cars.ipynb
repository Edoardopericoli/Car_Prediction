{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Classification and Generation\n",
    "\n",
    "[Martina Cioffi](https://github.com/martinacioffi) – 3010036\n",
    "\n",
    "[Edoardo Manieri](https://github.com/edoardomanieri) – 3084469\n",
    "\n",
    "[Valentina Parietti](https://github.com/ValentinaParietti) – 3007385\n",
    "\n",
    "[Edoardo Pericoli](https://github.com/Edoardopericoli) –  3001596\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Datasets](#datasets)\n",
    "    1. [Stanford Dataset](#stanford)\n",
    "    2. [Our Dataset](#our)\n",
    "\n",
    "2. [Classification](#classification)\n",
    "    1. [From Scratch](#scratch)\n",
    "    2. [EfficientNet](#effnet)\n",
    "    3. [YOLO](#yolo)\n",
    "    \n",
    "3. [Generation](#generation)\n",
    "\n",
    "    1. [Data Preparation](#datapreparation)\n",
    "    2. [StyleGAN](#stylegan)\n",
    "\n",
    "4. [References](#refs)\n",
    "\n",
    "Please, note that the whole code, together with a more detailed explanation on how to run it can be found on GitHub at the following [link](https://github.com/Edoardopericoli/Car_Prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets <a name=\"datasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main idea for this project was to train a model to be able to classify cars starting from pictures of them. Following is a brief explanation of the steps followed both in terms of the collection and building of the datasets and of the model(s) used for classification. Moreover, we also generate some new images of cars starting from our own pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Stanford Dataset <a name=\"stanford\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first trial consisted in trying to predict the make, model and year of a car using images from the [Stanford Dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). This contains slightly more than 16,000 images of which, however, only half are labelled. Therefore, in order to train our initial model, we used those 8,144 images to which we added four classes corresponding to cars we were more familiar with, which we downloaded with [Fatkun Batch Download Images](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en) and annotated with bounding boxes using [VGG Image Annotator (VIA)](http://www.robots.ox.ac.uk/~vgg/software/via/). \n",
    "\n",
    "This last step was performed in prevision of the fact that, given that some images had a relatively small car in it, with the background occupying the largest portion of the picture, a possible improvement to the model could be attained by first having the model draw bounding boxes around the car, then crop the image (keeping only the first one in case the picture contained more than one car) and then feeding these cropped images rather than the original ones to the model.\n",
    "\n",
    "Before our addition, the model contained 196 classes; below is a graphical representation of the distirbution of the different brands. Note, however, that the graph below does not imply imbalance between the classes: indeed, we predict the car's model and year rather than merely the make. Still, given that cars of the same make are inevitably more similar between each other than cars from different makes, the picture is useful in understanding the difficulty of the task.\n",
    "\n",
    "Please, see the [Classification](#classification) section for a summary of the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/edoardo/Desktop/University/Cars_Classification/Car_Prediction/guess-make\n"
     ]
    }
   ],
   "source": [
    "cd guess-make/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1127 18:42:37.150186 140481023244096 deprecation.py:506] From /home/edoardo/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "2019-11-27 18:42:37.170421: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2019-11-27 18:42:37.170461: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2019-11-27 18:42:37.170500: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (edoardo-notebook): /proc/driver/nvidia/version does not exist\n",
      "2019-11-27 18:42:37.171001: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-11-27 18:42:37.195066: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394355000 Hz\n",
      "2019-11-27 18:42:37.195461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50c0aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-27 18:42:37.195491: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "W1127 18:42:46.435889 140481023244096 nn_ops.py:4283] Large dropout rate: 0.5125 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1127 18:42:46.653681 140481023244096 nn_ops.py:4283] Large dropout rate: 0.525 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1127 18:42:46.868530 140481023244096 nn_ops.py:4283] Large dropout rate: 0.5375 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1127 18:42:47.086692 140481023244096 nn_ops.py:4283] Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1127 18:42:47.302561 140481023244096 nn_ops.py:4283] Large dropout rate: 0.5625 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1127 18:43:30.970385 140481023244096 module_wrapper.py:139] From /home/edoardo/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      " * Serving Flask app \"app\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      "I1127 18:43:50.840566 140481023244096 _internal.py:122]  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "I1127 18:44:34.784582 140478230025984 _internal.py:122] 127.0.0.1 - - [27/Nov/2019 18:44:34] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "I1127 18:44:36.952335 140477970962176 _internal.py:122] 127.0.0.1 - - [27/Nov/2019 18:44:36] \"\u001b[37mGET /guess HTTP/1.1\u001b[0m\" 200 -\n",
      "I1127 18:44:40.890626 140478230025984 _internal.py:122] 127.0.0.1 - - [27/Nov/2019 18:44:40] \"\u001b[32mPOST /guess HTTP/1.1\u001b[0m\" 302 -\n",
      "I1127 18:44:40.957359 140477970962176 _internal.py:122] 127.0.0.1 - - [27/Nov/2019 18:44:40] \"\u001b[37mGET /?img_path=static%2Fimages%2F000002.jpg HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "!python3 app.py -path static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERIRE ESEMPIO DI MACCHINE ANNOTATE DA NOI CON BOUNDING BOXES (VALE runna con path giusto)\n",
    "#e avendo chiamato all_labels_final il csv con bounding boxes di qualsiasi macchina\n",
    "\n",
    "for x in range(8209, 8274):\n",
    "    im = np.array(Image.open(f'/Users/martinacioffi/PycharmProjects/cars/Car_Prediction/mini_cooper_clubman_2019___Google_Search/0{x}.jpg'), dtype=np.uint8)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(im)\n",
    "    ics = all_labels_final.bbox_x1[x-1]\n",
    "    y = all_labels_final.bbox_y1[x-1]\n",
    "    width = all_labels_final.bbox_x2[x-1]\n",
    "    heigth = all_labels_final.bbox_y2[x-1]\n",
    "    rect = patches.Rectangle((ics, y),width-ics, heigth-y,linewidth=1,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUI QUALCUNO CHE HA LE ANNOTATIONS (EDO P.?) dovrebbe runnare per il summary graph\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "brands = df['brand'].value_counts()\n",
    "plt.figure(figsize=(10,10))\n",
    "plot = sns.barplot(brands.values, brands.index, alpha=0.8, orient='h')\n",
    "plt.title('Distribution of Brands - Stanford Dataset', fontsize=16)\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('brands_stanford.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E VOLENDO ANCHE QUESTO PER FAR VEDERE CHE ALCUNI MODELLI SI RIPETONO PER ANNI DIVERSI\n",
    "\n",
    "brands = df['model'].value_counts()\n",
    "brands = brands[:20]\n",
    "plt.figure(figsize=(10,10))\n",
    "plot = sns.barplot(brands.values, brands.index, alpha=0.8, orient='h')\n",
    "plt.title('Distribution of Models - Stanford Dataset', fontsize=16)\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.xticks(np.arange(min(brands.values), max(brands.values)+1, 1.0))\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('models_stanford.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e anche, se vogliamo, un count per far vedere il num di classi e avg number of pics per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Our Dataset <a name=\"our\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given, however, that the Stanford dataset has relatively few images per class, and a very high number of classes, we decided to build a new dataset from scratch, containing (i) cars we were more familiar with (i.e. mostly sold in Europe rather than in the United States), and (ii) more images per class (eventually, we had around 200 images on average per each car's model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STESSA COSA CON NOSTRI LABEL FINALI\n",
    "\n",
    "brands = df['brand'].value_counts()\n",
    "plt.figure(figsize=(10,10))\n",
    "plot = sns.barplot(brands.values, brands.index, alpha=0.8, orient='h')\n",
    "plt.title('Distribution of Brands - Our Dataset', fontsize=16)\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('brands_our.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification <a name=\"classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.From Scratch <a name=\"scratch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. EfficientNet <a name=\"effnt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. YOLO <a name=\"yolo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORSE LA STORIA DELLE BOUNDING BOXES FATTA DA NOI STA MEGLIO QUA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation <a name=\"generation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Preparation <a name=\"datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read raw images \n",
    "\n",
    "files = os.listdir('data/raw_data/StyleGAN/StyleGAN_raw')\n",
    "files.sort()\n",
    "files=files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that adds white borders to non square images and rescales them to 256x256\n",
    "\n",
    "def make_square(im, min_size=256, fill_color=(255, 255, 255, 0)):\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the make_square function to the raw images and create a folder with the final images \n",
    "\n",
    "for i in files:\n",
    "    im = Image.open('data/raw_data/StyleGAN/StyleGAN_raw/'+str(i))\n",
    "    new_im=make_square(im)\n",
    "    new_size=(256,256)\n",
    "    new_im = new_im.resize(new_size)\n",
    "    new_im.save('data/raw_data/StyleGAN/StyleGAN_final/'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan'...\n",
      "remote: Enumerating objects: 419, done.\u001b[K\n",
      "remote: Total 419 (delta 0), reused 0 (delta 0), pack-reused 419\u001b[K\n",
      "Receiving objects: 100% (419/419), 20.69 MiB | 3.60 MiB/s, done.\n",
      "Resolving deltas: 100% (245/245), done.\n"
     ]
    }
   ],
   "source": [
    "#Clone the repository needed to generate cars from our dataset\n",
    "\n",
    "!git clone https://github.com/ValentinaParietti/stylegan.git #This repository has been forked from the\n",
    "                                                             #original StyleGAN repository and some changes have\n",
    "                                                             #been made to it in order to run StyleGAN on our dataset. \n",
    "                                                             #More on this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"data/raw_data/StyleGAN/StyleGAN_final\"\n",
      "Creating dataset \"stylegan/datasets/custom_datasets\"\n",
      "Added 2108 images.                      \n"
     ]
    }
   ],
   "source": [
    "#Run the following command to convert the images into .tfrecords (format required by StyleGAN)\n",
    "\n",
    "!python stylegan/dataset_tool.py create_from_images stylegan/datasets/custom_datasets data/raw_data/StyleGAN/StyleGAN_final\n",
    "\n",
    "###COMMENT FOR US: the folder datasets should not be uploaded to the repo when pushing it (too heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: datasets/ (stored 0%)\n",
      "  adding: datasets/custom_datasets/ (stored 0%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r04.tfrecords (deflated 38%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r02.tfrecords (deflated 48%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r08.tfrecords (deflated 48%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r05.tfrecords (deflated 40%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r03.tfrecords (deflated 41%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r06.tfrecords (deflated 43%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r07.tfrecords (deflated 47%)\n"
     ]
    }
   ],
   "source": [
    "#Zip the newly filled datasets folder\n",
    "\n",
    "os.chdir('stylegan')\n",
    "!zip -r datasets_zip datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. StyleGAN <a name=\"stylegan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train StyleGAN and generate new images a GPU is needed.\n",
    "\n",
    "Therefore, the rest of the code for our generation task can be found on the Google Colab Notebook at this link: https://colab.research.google.com/drive/1FE9GBqh0qBQ8nUDDIDjWhy5R2sdgiqD0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References <a name=\"refs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**3D Object Representations for Fine-Grained Categorization**](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei.\n",
    "*4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13).* Sydney, Australia. Dec. 8, 2013.\n",
    "\n",
    "[**A Style-Based Generator Architecture for Generative Adversarial Networks**](https://arxiv.org/abs/1812.04948). Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)\n",
    "\n",
    "[**EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks**](https://arxiv.org/abs/1905.11946). Mingxing Tan, Quoc V. Le (Google Research, Brain Team, Mountain View, CA.)\n",
    "\n",
    "[**YOLOv3: An Incremental Improvement**](https://arxiv.org/abs/1804.02767). Joseph Redmon, Ali Farhadi. 2018. *arXiv:1804.02767*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
