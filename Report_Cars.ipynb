{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Classification and Generation\n",
    "\n",
    "[Martina Cioffi](https://github.com/martinacioffi) – 3010036\n",
    "\n",
    "[Edoardo Manieri](https://github.com/edoardomanieri) – 3084469\n",
    "\n",
    "[Valentina Parietti](https://github.com/ValentinaParietti) – 3007385\n",
    "\n",
    "[Edoardo Pericoli](https://github.com/Edoardopericoli) –  3001596\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "1. [Datasets](#datasets)\n",
    "    1. [Stanford Dataset](#stanford)\n",
    "    2. [Our Dataset](#our)\n",
    "\n",
    "2. [Classification](#classification)\n",
    "    1. [From Scratch](#scratch)\n",
    "    2. [EfficientNet](#effnet)\n",
    "    3. [YOLO](#yolo)\n",
    "    \n",
    "3. [Generation](#generation)\n",
    "\n",
    "    1. [Data Preparation](#datapreparation)\n",
    "    2. [StyleGAN](#stylegan)\n",
    "\n",
    "4. [References](#refs)\n",
    "\n",
    "Please, note that the whole code, together with a more detailed explanation on how to run it can be found on GitHub at the following [link](https://github.com/Edoardopericoli/Car_Prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets <a name=\"datasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main idea for this project was to train a model to be able to classify cars starting from pictures of them. Following is a brief explanation of the steps followed both in terms of the collection and building of the datasets and of the model(s) used for classification. Moreover, we also generate some new images of cars starting from our own pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Stanford Dataset <a name=\"stanford\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first trial consisted in trying to predict the make, model and year of a car using images from the [Stanford Dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). This contains slightly more than 16,000 images of which, however, only half are labelled. Therefore, in order to train our initial model, we used those 8,144 images to which we added four classes corresponding to cars we were more familiar with, which we downloaded with [Fatkun Batch Download Images](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en) and annotated with bounding boxes using [VGG Image Annotator (VIA)](http://www.robots.ox.ac.uk/~vgg/software/via/). \n",
    "\n",
    "This last step was performed in prevision of the fact that, given that some images had a relatively small car in it, with the background occupying the largest portion of the picture, a possible improvement to the model could be attained by first having the model draw bounding boxes around the car, then crop the image (keeping only the first one in case the picture contained more than one car) and then feeding these cropped images rather than the original ones to the model.\n",
    "\n",
    "Before our addition, the model contained 196 classes; below is a graphical representation of the distirbution of the different brands. Note, however, that the graph below does not imply imbalance between the classes: indeed, we predict the car's model and year rather than merely the make. Still, given that cars of the same make are inevitably more similar between each other than cars from different makes, the picture is useful in understanding the difficulty of the task.\n",
    "\n",
    "Please, see the [Classification](#classification) section for a summary of the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERIRE ESEMPIO DI MACCHINE ANNOTATE DA NOI CON BOUNDING BOXES (VALE runna con path giusto)\n",
    "#e avendo chiamato all_labels_final il csv con bounding boxes di qualsiasi macchina\n",
    "\n",
    "for x in range(8209, 8274):\n",
    "    im = np.array(Image.open(f'/Users/martinacioffi/PycharmProjects/cars/Car_Prediction/mini_cooper_clubman_2019___Google_Search/0{x}.jpg'), dtype=np.uint8)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(im)\n",
    "    ics = all_labels_final.bbox_x1[x-1]\n",
    "    y = all_labels_final.bbox_y1[x-1]\n",
    "    width = all_labels_final.bbox_x2[x-1]\n",
    "    heigth = all_labels_final.bbox_y2[x-1]\n",
    "    rect = patches.Rectangle((ics, y),width-ics, heigth-y,linewidth=1,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUI QUALCUNO CHE HA LE ANNOTATIONS (EDO P.?) dovrebbe runnare per il summary graph\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "brands = df['brand'].value_counts()\n",
    "plt.figure(figsize=(10,10))\n",
    "plot = sns.barplot(brands.values, brands.index, alpha=0.8, orient='h')\n",
    "plt.title('Distribution of Brands - Stanford Dataset', fontsize=16)\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('brands_stanford.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E VOLENDO ANCHE QUESTO PER FAR VEDERE CHE ALCUNI MODELLI SI RIPETONO PER ANNI DIVERSI\n",
    "\n",
    "brands = df['model'].value_counts()\n",
    "brands = brands[:20]\n",
    "plt.figure(figsize=(10,10))\n",
    "plot = sns.barplot(brands.values, brands.index, alpha=0.8, orient='h')\n",
    "plt.title('Distribution of Models - Stanford Dataset', fontsize=16)\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.xticks(np.arange(min(brands.values), max(brands.values)+1, 1.0))\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('models_stanford.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e anche, se vogliamo, un count per far vedere il num di classi e avg number of pics per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Our Dataset <a name=\"our\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given, however, that the Stanford dataset has relatively few images per class, and a very high number of classes, we decided to build a new dataset from scratch, containing (i) cars we were more familiar with (i.e. mostly sold in Europe rather than in the United States), and (ii) more images per class (eventually, we had around 200 images on average per each car's model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STESSA COSA CON NOSTRI LABEL FINALI\n",
    "\n",
    "brands = df['brand'].value_counts()\n",
    "plt.figure(figsize=(10,10))\n",
    "plot = sns.barplot(brands.values, brands.index, alpha=0.8, orient='h')\n",
    "plt.title('Distribution of Brands - Our Dataset', fontsize=16)\n",
    "plt.xlabel('Number of Occurrences', fontsize=12)\n",
    "plt.show()\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('brands_our.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification <a name=\"classification\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.From Scratch <a name=\"scratch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. EfficientNet <a name=\"effnt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. YOLO <a name=\"yolo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORSE LA STORIA DELLE BOUNDING BOXES FATTA DA NOI STA MEGLIO QUA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation <a name=\"generation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Preparation <a name=\"datapreparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read raw images \n",
    "\n",
    "files = os.listdir('data/raw_data/StyleGAN/StyleGAN_raw')\n",
    "files.sort()\n",
    "files=files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that adds white borders to non square images and rescales them to 256x256\n",
    "\n",
    "def make_square(im, min_size=256, fill_color=(255, 255, 255, 0)):\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the make_square function to the raw images and create a folder with the final images \n",
    "\n",
    "for i in files:\n",
    "    im = Image.open('data/raw_data/StyleGAN/StyleGAN_raw/'+str(i))\n",
    "    new_im=make_square(im)\n",
    "    new_size=(256,256)\n",
    "    new_im = new_im.resize(new_size)\n",
    "    new_im.save('data/raw_data/StyleGAN/StyleGAN_final/'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan'...\n",
      "remote: Enumerating objects: 419, done.\u001b[K\n",
      "remote: Total 419 (delta 0), reused 0 (delta 0), pack-reused 419\u001b[K\n",
      "Receiving objects: 100% (419/419), 20.69 MiB | 3.60 MiB/s, done.\n",
      "Resolving deltas: 100% (245/245), done.\n"
     ]
    }
   ],
   "source": [
    "#Clone the repository needed to generate cars from our dataset\n",
    "\n",
    "!git clone https://github.com/ValentinaParietti/stylegan.git #This repository has been forked from the\n",
    "                                                             #original StyleGAN repository and some changes have\n",
    "                                                             #been made to it in order to run StyleGAN on our dataset. \n",
    "                                                             #More on this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"data/raw_data/StyleGAN/StyleGAN_final\"\n",
      "Creating dataset \"stylegan/datasets/custom_datasets\"\n",
      "Added 2108 images.                      \n"
     ]
    }
   ],
   "source": [
    "#Run the following command to convert the images into .tfrecords (format required by StyleGAN)\n",
    "\n",
    "!python stylegan/dataset_tool.py create_from_images stylegan/datasets/custom_datasets data/raw_data/StyleGAN/StyleGAN_final\n",
    "\n",
    "###COMMENT FOR US: the folder datasets should not be uploaded to the repo when pushing it (too heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: datasets/ (stored 0%)\n",
      "  adding: datasets/custom_datasets/ (stored 0%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r04.tfrecords (deflated 38%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r02.tfrecords (deflated 48%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r08.tfrecords (deflated 48%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r05.tfrecords (deflated 40%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r03.tfrecords (deflated 41%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r06.tfrecords (deflated 43%)\n",
      "  adding: datasets/custom_datasets/custom_datasets-r07.tfrecords (deflated 47%)\n"
     ]
    }
   ],
   "source": [
    "#Zip the newly filled datasets folder\n",
    "\n",
    "os.chdir('stylegan')\n",
    "!zip -r datasets_zip datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. StyleGAN <a name=\"stylegan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train StyleGAN and generate new images a GPU is needed.\n",
    "\n",
    "Therefore, the rest of the code for our generation task can be found on the Google Colab Notebook at this link: https://colab.research.google.com/drive/1FE9GBqh0qBQ8nUDDIDjWhy5R2sdgiqD0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References <a name=\"refs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**3D Object Representations for Fine-Grained Categorization**](https://ai.stanford.edu/~jkrause/cars/car_dataset.html). Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei.\n",
    "*4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13).* Sydney, Australia. Dec. 8, 2013.\n",
    "\n",
    "[**A Style-Based Generator Architecture for Generative Adversarial Networks**](https://arxiv.org/abs/1812.04948). Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)\n",
    "\n",
    "[**EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks**](https://arxiv.org/abs/1905.11946). Mingxing Tan, Quoc V. Le (Google Research, Brain Team, Mountain View, CA.)\n",
    "\n",
    "[**YOLOv3: An Incremental Improvement**](https://arxiv.org/abs/1804.02767). Joseph Redmon, Ali Farhadi. 2018. *arXiv:1804.02767*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
